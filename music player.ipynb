{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d773988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'happy': 1, 'sad': 2}\n",
      "['angry', 'happy', 'sad']\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "\n",
    "data_path='emotions'\n",
    "categories=os.listdir(data_path)\n",
    "labels=[i for i in range(len(categories))]\n",
    "\n",
    "label_dict=dict(zip(categories,labels)) #empty dictionary\n",
    "\n",
    "print(label_dict)\n",
    "print(categories)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd439b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=100\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "facedata = \"haarcascade_frontalface_alt.xml\"\n",
    "cascade = cv2.CascadeClassifier(facedata)\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "        faces = cascade.detectMultiScale(img) \n",
    "        try:\n",
    "            for f in faces:\n",
    "                x, y, w, h = [v for v in f]\n",
    "                sub_face = img[y:y + h, x:x + w]\n",
    "                gray=cv2.cvtColor(sub_face,cv2.COLOR_BGR2GRAY)           \n",
    "                #Coverting the image into gray scale\n",
    "                resized=cv2.resize(gray,(img_size,img_size))\n",
    "                data.append(resized)\n",
    "                target.append(label_dict[category])\n",
    "        except Exception as e:\n",
    "            print('Exception:',e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(data)/255.0\n",
    "data=np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "target=np.array(target)\n",
    "from keras.utils import np_utils\n",
    "new_target=np_utils.to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bece84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'C:\\Users\\91965\\emotion based music player\\emotions\\data',data)\n",
    "np.save(r'C:\\Users\\91965\\emotion based music player\\emotions\\target',new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316c4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load(r'C:\\Users\\91965\\emotion based music player\\emotions\\data.npy')\n",
    "target=np.load(r'C:\\Users\\91965\\emotion based music player\\emotions\\target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ab6136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 98, 98, 200)       2000      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 98, 98, 200)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 49, 49, 200)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 47, 47, 100)       180100    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 47, 47, 100)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 23, 23, 100)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 52900)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 52900)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                2645050   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,827,303\n",
      "Trainable params: 2,827,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(200,(3,3),input_shape=data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The first CNN layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Conv2D(100,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "#Flatten layer to stack the output convolutions from second convolution layer\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b2138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5c20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6461fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5193, 100, 100, 1)\n",
      "(5193, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db8651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 1.0963 - accuracy: 0.3893INFO:tensorflow:Assets written to: ./training\\model001.model\\assets\n",
      "130/130 [==============================] - 125s 952ms/step - loss: 1.0963 - accuracy: 0.3893 - val_loss: 1.0918 - val_accuracy: 0.4187\n",
      "Epoch 2/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 1.0624 - accuracy: 0.4458INFO:tensorflow:Assets written to: ./training\\model002.model\\assets\n",
      "130/130 [==============================] - 128s 985ms/step - loss: 1.0624 - accuracy: 0.4458 - val_loss: 1.0232 - val_accuracy: 0.4928\n",
      "Epoch 3/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.9467 - accuracy: 0.5272INFO:tensorflow:Assets written to: ./training\\model003.model\\assets\n",
      "130/130 [==============================] - 121s 932ms/step - loss: 0.9467 - accuracy: 0.5272 - val_loss: 0.8700 - val_accuracy: 0.5602\n",
      "Epoch 4/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.8704 - accuracy: 0.5821INFO:tensorflow:Assets written to: ./training\\model004.model\\assets\n",
      "130/130 [==============================] - 122s 943ms/step - loss: 0.8704 - accuracy: 0.5821 - val_loss: 0.8229 - val_accuracy: 0.6352\n",
      "Epoch 5/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.8376 - accuracy: 0.6122INFO:tensorflow:Assets written to: ./training\\model005.model\\assets\n",
      "130/130 [==============================] - 117s 902ms/step - loss: 0.8376 - accuracy: 0.6122 - val_loss: 0.8226 - val_accuracy: 0.6218\n",
      "Epoch 6/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.7916 - accuracy: 0.6305INFO:tensorflow:Assets written to: ./training\\model006.model\\assets\n",
      "130/130 [==============================] - 125s 963ms/step - loss: 0.7916 - accuracy: 0.6305 - val_loss: 0.7842 - val_accuracy: 0.6323\n",
      "Epoch 7/55\n",
      "130/130 [==============================] - 129s 993ms/step - loss: 0.7689 - accuracy: 0.6558 - val_loss: 0.8211 - val_accuracy: 0.6227\n",
      "Epoch 8/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.7476 - accuracy: 0.6647INFO:tensorflow:Assets written to: ./training\\model008.model\\assets\n",
      "130/130 [==============================] - 123s 943ms/step - loss: 0.7476 - accuracy: 0.6647 - val_loss: 0.7523 - val_accuracy: 0.6506\n",
      "Epoch 9/55\n",
      "130/130 [==============================] - 130s 1s/step - loss: 0.7239 - accuracy: 0.6810 - val_loss: 0.7575 - val_accuracy: 0.6554\n",
      "Epoch 10/55\n",
      "130/130 [==============================] - 134s 1s/step - loss: 0.7012 - accuracy: 0.6844 - val_loss: 0.7697 - val_accuracy: 0.6439\n",
      "Epoch 11/55\n",
      "130/130 [==============================] - 122s 937ms/step - loss: 0.6900 - accuracy: 0.6943 - val_loss: 0.7580 - val_accuracy: 0.6352\n",
      "Epoch 12/55\n",
      "130/130 [==============================] - 141s 1s/step - loss: 0.6788 - accuracy: 0.6960 - val_loss: 0.7878 - val_accuracy: 0.6295\n",
      "Epoch 13/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.6580 - accuracy: 0.7044INFO:tensorflow:Assets written to: ./training\\model013.model\\assets\n",
      "130/130 [==============================] - 133s 1s/step - loss: 0.6580 - accuracy: 0.7044 - val_loss: 0.7242 - val_accuracy: 0.6785\n",
      "Epoch 14/55\n",
      "130/130 [==============================] - 131s 1s/step - loss: 0.6400 - accuracy: 0.7159 - val_loss: 0.7493 - val_accuracy: 0.6449\n",
      "Epoch 15/55\n",
      "130/130 [==============================] - 136s 1s/step - loss: 0.6275 - accuracy: 0.7282 - val_loss: 0.7327 - val_accuracy: 0.6708\n",
      "Epoch 16/55\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.7383INFO:tensorflow:Assets written to: ./training\\model016.model\\assets\n",
      "130/130 [==============================] - 134s 1s/step - loss: 0.6069 - accuracy: 0.7383 - val_loss: 0.7183 - val_accuracy: 0.6718\n",
      "Epoch 17/55\n",
      "130/130 [==============================] - 132s 1s/step - loss: 0.5891 - accuracy: 0.7482 - val_loss: 0.7672 - val_accuracy: 0.6420\n",
      "Epoch 18/55\n",
      "130/130 [==============================] - 131s 1s/step - loss: 0.5825 - accuracy: 0.7499 - val_loss: 0.7366 - val_accuracy: 0.6785\n",
      "Epoch 19/55\n",
      "130/130 [==============================] - 133s 1s/step - loss: 0.5591 - accuracy: 0.7576 - val_loss: 0.7291 - val_accuracy: 0.6603\n",
      "Epoch 20/55\n",
      "130/130 [==============================] - 125s 963ms/step - loss: 0.5464 - accuracy: 0.7643 - val_loss: 0.7738 - val_accuracy: 0.6679\n",
      "Epoch 21/55\n",
      "130/130 [==============================] - 121s 932ms/step - loss: 0.5343 - accuracy: 0.7650 - val_loss: 0.7378 - val_accuracy: 0.6699\n",
      "Epoch 22/55\n",
      "130/130 [==============================] - 121s 934ms/step - loss: 0.5215 - accuracy: 0.7809 - val_loss: 0.7388 - val_accuracy: 0.6747\n",
      "Epoch 23/55\n",
      "130/130 [==============================] - 121s 931ms/step - loss: 0.5038 - accuracy: 0.7824 - val_loss: 0.7522 - val_accuracy: 0.6708\n",
      "Epoch 24/55\n",
      "130/130 [==============================] - 121s 930ms/step - loss: 0.4879 - accuracy: 0.7963 - val_loss: 0.7513 - val_accuracy: 0.6776\n",
      "Epoch 25/55\n",
      "130/130 [==============================] - 121s 933ms/step - loss: 0.4804 - accuracy: 0.7990 - val_loss: 0.7613 - val_accuracy: 0.6622\n",
      "Epoch 26/55\n",
      "130/130 [==============================] - 121s 930ms/step - loss: 0.4681 - accuracy: 0.8084 - val_loss: 0.7904 - val_accuracy: 0.6506\n",
      "Epoch 27/55\n",
      "130/130 [==============================] - 121s 930ms/step - loss: 0.4683 - accuracy: 0.8060 - val_loss: 0.7805 - val_accuracy: 0.6670\n",
      "Epoch 28/55\n",
      "130/130 [==============================] - 128s 983ms/step - loss: 0.4459 - accuracy: 0.8139 - val_loss: 0.7877 - val_accuracy: 0.6631\n",
      "Epoch 29/55\n",
      "130/130 [==============================] - 121s 930ms/step - loss: 0.4288 - accuracy: 0.8204 - val_loss: 0.8042 - val_accuracy: 0.6824\n",
      "Epoch 30/55\n",
      "130/130 [==============================] - 120s 927ms/step - loss: 0.4404 - accuracy: 0.8168 - val_loss: 0.7751 - val_accuracy: 0.6824\n",
      "Epoch 31/55\n",
      "130/130 [==============================] - 121s 929ms/step - loss: 0.4235 - accuracy: 0.8296 - val_loss: 0.7674 - val_accuracy: 0.6747\n",
      "Epoch 32/55\n",
      "130/130 [==============================] - 121s 929ms/step - loss: 0.4124 - accuracy: 0.8322 - val_loss: 0.8390 - val_accuracy: 0.6641\n",
      "Epoch 33/55\n",
      "130/130 [==============================] - 121s 928ms/step - loss: 0.4091 - accuracy: 0.8382 - val_loss: 0.7836 - val_accuracy: 0.6805\n",
      "Epoch 34/55\n",
      "130/130 [==============================] - 121s 935ms/step - loss: 0.4047 - accuracy: 0.8317 - val_loss: 0.8180 - val_accuracy: 0.6641\n",
      "Epoch 35/55\n",
      "130/130 [==============================] - 1133s 9s/step - loss: 0.3994 - accuracy: 0.8382 - val_loss: 0.7874 - val_accuracy: 0.6766\n",
      "Epoch 36/55\n",
      "130/130 [==============================] - 129s 993ms/step - loss: 0.3956 - accuracy: 0.8296 - val_loss: 0.8103 - val_accuracy: 0.6699\n",
      "Epoch 37/55\n",
      "130/130 [==============================] - 121s 930ms/step - loss: 0.3607 - accuracy: 0.8534 - val_loss: 0.8428 - val_accuracy: 0.6737\n",
      "Epoch 38/55\n",
      "130/130 [==============================] - 122s 938ms/step - loss: 0.3740 - accuracy: 0.8459 - val_loss: 0.8266 - val_accuracy: 0.6699\n",
      "Epoch 39/55\n",
      "130/130 [==============================] - 122s 938ms/step - loss: 0.3590 - accuracy: 0.8584 - val_loss: 0.8306 - val_accuracy: 0.6737\n",
      "Epoch 40/55\n",
      "130/130 [==============================] - 128s 988ms/step - loss: 0.3393 - accuracy: 0.8657 - val_loss: 0.8299 - val_accuracy: 0.6651\n",
      "Epoch 41/55\n",
      "130/130 [==============================] - 122s 939ms/step - loss: 0.3553 - accuracy: 0.8565 - val_loss: 0.8774 - val_accuracy: 0.6670\n",
      "Epoch 42/55\n",
      "130/130 [==============================] - 124s 955ms/step - loss: 0.3394 - accuracy: 0.8652 - val_loss: 0.7948 - val_accuracy: 0.6718\n",
      "Epoch 43/55\n",
      "130/130 [==============================] - 123s 946ms/step - loss: 0.3321 - accuracy: 0.8676 - val_loss: 0.8625 - val_accuracy: 0.6660\n",
      "Epoch 44/55\n",
      "130/130 [==============================] - 130s 1s/step - loss: 0.3357 - accuracy: 0.8633 - val_loss: 0.8375 - val_accuracy: 0.6756\n",
      "Epoch 45/55\n",
      "130/130 [==============================] - 140s 1s/step - loss: 0.3323 - accuracy: 0.8669 - val_loss: 0.8441 - val_accuracy: 0.6708\n",
      "Epoch 46/55\n",
      "130/130 [==============================] - 135s 1s/step - loss: 0.3134 - accuracy: 0.8727 - val_loss: 0.8733 - val_accuracy: 0.6689\n",
      "Epoch 47/55\n",
      "130/130 [==============================] - 122s 942ms/step - loss: 0.3106 - accuracy: 0.8763 - val_loss: 0.9113 - val_accuracy: 0.6708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/55\n",
      "130/130 [==============================] - 131s 1s/step - loss: 0.3055 - accuracy: 0.8784 - val_loss: 0.8384 - val_accuracy: 0.6776\n",
      "Epoch 49/55\n",
      "130/130 [==============================] - 141s 1s/step - loss: 0.3145 - accuracy: 0.8724 - val_loss: 0.8565 - val_accuracy: 0.6728\n",
      "Epoch 50/55\n",
      "130/130 [==============================] - 122s 940ms/step - loss: 0.2914 - accuracy: 0.8885 - val_loss: 0.9068 - val_accuracy: 0.6641\n",
      "Epoch 51/55\n",
      "130/130 [==============================] - 125s 959ms/step - loss: 0.2902 - accuracy: 0.8857 - val_loss: 0.9116 - val_accuracy: 0.6660\n",
      "Epoch 52/55\n",
      "130/130 [==============================] - 122s 940ms/step - loss: 0.2787 - accuracy: 0.8917 - val_loss: 0.9211 - val_accuracy: 0.6641\n",
      "Epoch 53/55\n",
      "130/130 [==============================] - 126s 966ms/step - loss: 0.2731 - accuracy: 0.8948 - val_loss: 0.9730 - val_accuracy: 0.6843\n",
      "Epoch 54/55\n",
      "130/130 [==============================] - 124s 958ms/step - loss: 0.2752 - accuracy: 0.8953 - val_loss: 0.9237 - val_accuracy: 0.6795\n",
      "Epoch 55/55\n",
      "130/130 [==============================] - 126s 972ms/step - loss: 0.2611 - accuracy: 0.8960 - val_loss: 0.9404 - val_accuracy: 0.6679\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('./training/model{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto') \n",
    "history=model.fit(train_data,train_target,epochs=55,callbacks=checkpoint,validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed1c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a14db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4220cd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 4s 212ms/step - loss: 1.0999 - accuracy: 0.3253\n",
      "[1.0999375581741333, 0.3252595067024231]\n",
      "(578, 100, 100, 1)\n",
      "(578, 3)\n"
     ]
    }
   ],
   "source": [
    "print(reconstructed_model.evaluate(test_data,test_target))\n",
    "print(test_data.shape)\n",
    "print(test_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5d19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=reconstructed_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf0f86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0\n",
      " 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0\n",
      " 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0\n",
      " 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2\n",
      " 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 2\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 0 0\n",
      " 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "label=np.argmax(prediction,axis=1)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165e2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bf882be",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "labels_dict={0:'Angry',1:'happy',2:'Sad'}\n",
    "\n",
    "color_dict={0:(0,0,255),1:(0,255,0),2:(255,0,0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1cf1268",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_clsfr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-231262cd68cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mgray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mfaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_clsfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'face_clsfr' is not defined"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "currentframe = 0\n",
    "import random\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "while(True):\n",
    "    \n",
    "    ret,img=cap.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_clsfr.detectMultiScale(gray,1.3,3)  \n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "    \n",
    "        face_img=gray[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,100,100,1))\n",
    "        result=reconstructed_model.predict(reshaped)\n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "        \n",
    "    cv2.imshow('LIVE',img)\n",
    "  \n",
    "    #out.write(img)\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "    if ret==False:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "if labels_dict[label]  == 'angry':\n",
    "    randomfile = random.choice(os.listdir(r\"C:\\Users\\91965\\project\\songs\\angry\"))\n",
    "    print('You are angry !!!!')\n",
    "    file=os.path.join(r\"C:\\Users\\91965\\project\\songs\\angry\",randomfile)\n",
    "if labels_dict[label]  == 'happy':\n",
    "    randomfile = random.choice(os.listdir(r\"C:\\Users\\91965\\project\\songs\\happy\"))\n",
    "    print('You are smiling :') \n",
    "    file=os.path.join(r\"C:\\Users\\91965\\project\\songs\\happy\",randomfile)\n",
    "    print(file)\n",
    "if labels_dict[label]  == 'Sad':\n",
    "    randomfile = random.choice(os.listdir(r\"C:\\Users\\91965\\project\\songs\\sad\"))\n",
    "    print('You are sad,dont worry:')\n",
    "    file=os.path.join(r\"C:\\Users\\91965\\project\\songs\\sad\",randomfile)\n",
    "ipd.Audio(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28582dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    }
   ],
   "source": [
    "print(labels_dict[label] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a3163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
